{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7942ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae20e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single node of the tree - either Decision or leaf\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, info_gain = None, value=None):\n",
    "        \n",
    "        #only for the decision nodes\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # only for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b863d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to build the whole decision tree\n",
    "class Decision_tree():\n",
    "    def __init__(self, max_depth=3, min_sample_split=2, gain_fn=\"gini\",n_features=None):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "        max_depth - int, the maximum depth of the tree\n",
    "        min_sample_split - int, the minimum number of splits on the given data\n",
    "        \"\"\"\n",
    "        self.gain_fn = gain_fn\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def optimal_tree(self, data, curr_depth=0):\n",
    "        # extract the data\n",
    "        X, y = data[:,:-1], data[:,-1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # feature selection if number of features are gven\n",
    "        self.n_features = n_features if self.n_features is None else np.minimum(self.n_features, n_features)\n",
    "        \n",
    "        if n_samples > self.min_sample_split and curr_depth <= self.max_depth:\n",
    "            # to find the best decision split for the node\n",
    "            feat_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
    "            \n",
    "            best_decision = self.get_best_decision(data, n_samples, feat_idxs) # A dict\n",
    "            \n",
    "            if best_decision[\"entropy_gain\"] > 0:\n",
    "                # building the left tree of the parent node\n",
    "                left_subtree = self.optimal_tree(best_decision[\"dataset_left\"], curr_depth+1)\n",
    "                # building the right tree of the parent node\n",
    "                right_subtree = self.optimal_tree(best_decision[\"dataset_right\"], curr_depth+1)\n",
    "                \n",
    "                # forming the tree using decision nodes\n",
    "                return Node(best_decision[\"feature_idx\"], best_decision[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_decision[\"entropy_gain\"])\n",
    "            \n",
    "        value = self.calculate_leaf_value(y)\n",
    "            \n",
    "        # inserting the leaf nodes\n",
    "        return Node(value=value)\n",
    "            \n",
    "            \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key = Y.count)\n",
    "    \n",
    "    def get_best_decision(self, data, n_samples, n_features_idx):\n",
    "        \n",
    "            best_decision = {}\n",
    "            max_entropy_gain = -float(\"inf\")\n",
    "            \n",
    "            for feature_idx in n_features_idx:\n",
    "                feature = data[:,feature_idx]\n",
    "                \n",
    "                poss_thres = np.unique(feature)\n",
    "            \n",
    "                for tres in poss_thres:\n",
    "                    data_left, data_right = self.data_split_thresh(data,feature_idx, tres)\n",
    "\n",
    "                    if len(data_left) > 0 and len(data_right) > 0:\n",
    "                        y,y_left,y_right = data[:,:-1], data_left[:,:-1], data_right[:,:-1]\n",
    "\n",
    "                        curr_entropy_gain = self.entropy_gain(y, y_left, y_right,self.gain_fn)\n",
    "\n",
    "                        if curr_entropy_gain > max_entropy_gain:\n",
    "                            best_decision[\"feature_idx\"] = feature_idx\n",
    "                            best_decision[\"threshold\"] = tres\n",
    "                            best_decision[\"dataset_left\"] = data_left\n",
    "                            best_decision[\"dataset_right\"] = data_right\n",
    "                            best_decision[\"entropy_gain\"] = curr_entropy_gain\n",
    "                            max_entropy_gain = curr_entropy_gain\n",
    "        \n",
    "            return best_decision\n",
    "        \n",
    "    def data_split_thresh(self,data, feature_idx, tres):\n",
    "        \n",
    "        data_left = [row for row in data if row[feature_idx] <= tres]\n",
    "        data_right = [row for row in data if row[feature_idx] > tres]\n",
    "        return np.array(data_left), np.array(data_right)\n",
    "    \n",
    "    def entropy_gain(self, des_node, l_child, r_child, mode=\"entropy\"):\n",
    "        \n",
    "        w_l = len(l_child)/len(des_node)\n",
    "        w_r = len(r_child)/len(des_node)\n",
    "        \n",
    "        if mode == \"gini\":\n",
    "            info_gain = self.gini_impurity(des_node) - ((w_l * self.gini_impurity(l_child)) + (w_r * self.gini_impurity(r_child)))\n",
    "            \n",
    "        else:\n",
    "            info_gain = self.entropy(des_node) - ((w_l * self.entropy(l_child)) + (w_r * self.entropy(r_child)))\n",
    "        return info_gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        '''\n",
    "        A function to find the entropy gain\n",
    "        '''\n",
    "        unique_idx = np.unique(y)\n",
    "        entropy = 0\n",
    "        for idx in unique_idx:\n",
    "            p_y_ = len(y[y == idx]) / len(y)\n",
    "            entropy += -(p_y_)*np.log2(p_y_)\n",
    "            \n",
    "        return entropy\n",
    "    \n",
    "    def gini_impurity(self, y):\n",
    "        \"\"\"\n",
    "        A function to find the gini impurities\n",
    "        \"\"\"\n",
    "        unique_idx = np.unique(y)\n",
    "        gini_imp = 1\n",
    "        for idx in unique_idx:\n",
    "            p_y_ = len(y[y == idx]) / len(y)\n",
    "            gini_imp -= (p_y_ ** 2)\n",
    "            \n",
    "        return gini_imp\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left_child, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right_child, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # Training for the optimal fit\n",
    "        data = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.optimal_tree(data)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predicting the testing data\n",
    "        pred_labels = [int(self.decision_trees_pred(x, self.root)) for x in X]\n",
    "        \n",
    "        return pred_labels\n",
    "    \n",
    "    def decision_trees_pred(self, X, tree):\n",
    "        '''Recursive method to predict from the tree'''\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        \n",
    "        feature_val = X[tree.feature_index]\n",
    "        \n",
    "        if feature_val<= tree.threshold:\n",
    "            return self.decision_trees_pred(X, tree.left_child)\n",
    "        \n",
    "        else:\n",
    "            return self.decision_trees_pred(X, tree.right_child)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
